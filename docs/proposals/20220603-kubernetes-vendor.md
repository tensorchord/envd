# Multi-runtime Support

Authors:
- [Ce Gao](https://github.com/gaocegege)

## Summary

envd builds and runs the environments with Docker. This proposal is to support Kubernetes as a runtime option. After this feature, envd can build the environment with Docker and run on Kubernetes.

## Motivation

Docker runtime only works for individual developers. There are many data scientists developing models on Kubernetes. In the current design, they may need to run `envd build`, then push the image to a OCI image registry manually. The image can be used after a deployment or a pod is created in Kubernetes.

We can provide native support for ease of use.

## Goals

- Run environments on Kubernetes
  - Build the envd environments with the buildkitd pod
  - Compose Kubernetes resources (e.g. services, pods) when run `envd up`
  - Push the image to a registry
  - Port forward for sshd and jupyter notebook service
- Manage environments on Kubernetes
  - List environments running on Kubernetes
- Garbage collection
  - Delete idle pods

## Non-Goals

- Sync the pod file system with the local host file system.
  - It should be discussed in [tensorchord/envd#928](https://github.com/tensorchord/envd/discussions/928)
- Monitoring on Kubernetes
- Pause and unpause environments on Kubernetes (Kubernetes does not support the primitives)
- Manage envd images on Kubernetes

## Proposal

### User stories

#### CI/CD with envd

As a ML infra engineer, I want to build the images with CI/CD tools. Docker may not be set up in the CI/CD runners, so that envd should support to build with buildkitd running on the Kubernetes.

#### Remote development

As a AI/ML engineer, I want to develop on the remote cluster (managed by Kubernetes).

### CLI behavior

We need to support primitives like `run`:

```
$ envd run --env my-env --image my-image
```

User may use `envd` to build the image, and use it on Kubernetes. Thus they need to push the image to a registry. `envd up` may not satisfy the requirements.

The end-to-end process will be:

```
$ envd context create --name test --builder-name test --use --builder kube-pod --runner server --runner-addr http://localhost:2222
$ envd login
$ envd build
$ envd push
$ envd run --env test --image test

or 

$ envd context create --name test --builder-name test --use --builder kube-pod --runner server --runner-addr http://localhost:2222
$ envd login
$ envd up
```

The user first declares which runner will be used. `server` should be used if the user expects running on Kubernetes. The `server` is the short name of envd API server.

Then the user needs to login to the server with the public key.

After that, the user can manage the environments on Kubernetes.

### Design Details

#### envd API server

![](https://user-images.githubusercontent.com/5100735/190376297-f2ce5938-ac29-4eaf-8253-61461bcef8bd.png)

envd API server provides RESTful HTTP API to the envd CLI, and acts as the ssh proxy server (TCP), with the help of [libcontainerssh](https://github.com/ContainerSSH/libcontainerssh)

envd CLI communicates with the API server to:

- Register public key to the server for auth.
- Create backend environment to connect. If it is on Kubernetes, the backend environment will be run inside a pod.
- Connect to the ssh proxy server.

#### SSH auth keys

We embed an ssh server to the image and a pub key. And the key is specified with a CLI argument in `envd-sshd`.

```
/var/envd/bin/envd-sshd --authorized-keys <KEY_PATH> --port <PORT> --shell <SHELL>
```

It does not work with the Kubernetes design. The binary `envd-sshd` should read the environment variable `ENVD_SSH_KEY_PATH` to load the keys, instead of the CLI argument. The backend pods will be created with the same public key mounted in runtime. The public key is generated by the envd API server.

The envd API server checks if the public key and signature provided by the user is valid. If both are valid, the server will use the generated private key and public key to connect to the backend pod.

#### Reconnection

If the user reconnects to the pod, envd API server needs to know the correct pod to reconnect.

Thus the backend pod should have a unique identifier. When the user run envd attach (Or maybe envd ssh), the only information given to envd API server, is the username. Thus the CLI should encode the project name, the user, and other info, and use it as the username.

Or, the server grants the user a unique random username when the envd CLI logins. The envd CLI uses the unique username to communicate with the API server.

The main challenge here is to identify the correct pod with the existing SSH protocol.

#### Garbage collection

The pod can be deleted if it is idle within a given time threshold. This feature is related to the containerssh audit. A new daemon process `envd-server-gc` can be introduced. It watches the audit log file directory, then delete the idle pods.

#### Port forwarding

There are several ports will be used in envd:

- sshd server port. envd randomly selects a host port for sshd.
- jupyter notebook port.
- RStudio server port.

The section is to be added. SSH port is forwarded by the ssh proxy server. Other ports may need ssh tunneling.

#### Data and code integration

`runtime.docker.mount` and other docker-specific funcs will be ignored in kubernetes context. `runtime.kubernetes.mount()` should create/use the corresponding PV/PVC in the backend pod.

```python
runtime.kubernetes.mount("")
```

### Implementation details/Notes/Constraints

#### envd contexts

envd uses buildkitd container in the local docker host to build the images by default. `context` command will be introduced to support remote build.

```
NAME:
   envd context - Manage envd contexts

USAGE:
   envd context command [command options] [arguments...]

COMMANDS:
   create   Create envd context
   ls       List envd contexts
   rm       Remove envd context
   use      Use the specified envd context
   help, h  Shows a list of commands or help for one command

OPTIONS:
   --help, -h  show help (default: false)
```

The structure of the envd context is:

```go
type Context struct {
	Name          string      `json:"name,omitempty"`
	Builder       BuilderType `json:"builder,omitempty"`
	BuilderSocket string      `json:"builder_socket,omitempty"`

	Runner     RunnerType `json:"runner,omitempty"`
	RunnerAddr *string    `json:"runner_addr,omitempty"`
}

type BuilderType string

const (
	BuilderTypeDocker     BuilderType = "docker-container"
	BuilderTypeKubernetes BuilderType = "kube-pod"
)

type RunnerType string

const (
	RunnerTypeDocker     RunnerType = "docker"
	RunnerTypeKubernetes RunnerType = "server"
)
```

The default envd context is:

```go
Context{
    Name:          "default",
    Builder:       BuilderTypeDocker,
    BuilderSocket: "unix:///var/run/docker.sock",
    Runner:        RunnerTypeDocker,
    RunnerAddr:    nil,
}
```

Users may use `envd context create` to create new context and use it:

```
$ envd context create --name my-context --builder-type docker-container --runner-type docker --docker-host unix:///var/run/docker.sock
```

#### Runtime interface

A new interface `Runtime` will be introduced. And the interface `docker.Client` will be updated.

```go
type Runtime interface {
	StartEnvironment()

    ListEnvironments()
    GetEnvironment()
    PauseEnvironment()
    ResumeEnvironment()
    DestroyEnvironment()

    GPUEnabled()
}
```

`docker.Client` is used to:

- Destroy the environment via `envd destroy`
- Check if GPU is supported and run the docker container via `envd up`
- Load the image into the local docker host via `builder`
- Check if the buildkitd container is running in `buildkit.Client`
- List images, environments in `envd.Engine`

`envd destroy`, `envd up`, and `envd.Engine` should not use `docker.Client` any more. It should be migrated to `Runtime`. Because these func calls are related to runtime.

`builder` and `buildkit.Client` still needs to use `docker.Client` since we keep using docker to build the images.

The pseudocode of the new logic will be like:

```go
// Destroy the environment
func destroy(clicontext *cli.Context) error {
    runtime := runtime.New(getCLIFlag("runtime-vendor"))
    runtime.DestroyEnvironment(...)
}

func up(clicontext *cli.Context) error {
    // Build the image first
    builder.Build()
    // Push the image to a registry
    pushImage()
    runtime := runtime.New(getCLIFlag("runtime-vendor"))
    runtime.StartEnvironment(...)
}
```

### Test Plan

[kind](https://github.com/kubernetes-sigs/kind)-based integration test should be added. 
